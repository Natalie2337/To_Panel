# A Demo Fully Autogenerated by AI


"""
build_panel.py — Convert many wide-format indicator files into one Panel (long) dataset.

Assumptions
-----------
1) Each file corresponds to ONE indicator (variable). The file name (without extension) is used as the variable name.
2) The first column is the region name (e.g., '地区', '省份', 'region').
3) The remaining columns are years like '2023年', '2022年', ..., or '2010' (any 4-digit year, with/without the suffix '年').
4) Cells contain numeric values; missing/empty cells are allowed.
5) Files are .xlsx/.xls/.csv. (Excel requires openpyxl or xlrd installed in your environment.)

Usage
-----
python build_panel.py --input_dir ./data --output panel.xlsx \
    --region_col 地区 --pattern "*.xlsx" --clean_region yes

If your files include mixed types, you can run multiple times with different --pattern values.

Output
------
A long-format panel file with columns: ['region', 'year', <var1>, <var2>, ...]
Saved to CSV and/or Excel depending on the --output extension.
"""

import argparse
import re
import sys
from pathlib import Path
from typing import List, Tuple

import pandas as pd

YEAR_RE = re.compile(r'(19|20)\d{2}')  # capture a 4-digit year starting with 19xx or 20xx

def guess_year_columns(df: pd.DataFrame) -> List[str]:
    """Return columns that look like years (2010, '2010年', '2010 年', etc.)."""
    year_cols = []
    for c in df.columns:
        if c is None:
            continue
        s = str(c).strip()
        if YEAR_RE.search(s):
            year_cols.append(c)
    return year_cols

def normalize_year(s) -> int:
    """Extract the first 4-digit year and convert to int. Return None if not found."""
    if pd.isna(s):
        return None
    m = YEAR_RE.search(str(s))
    return int(m.group(0)) if m else None

def clean_region_name(name: str) -> str:
    """A conservative region normalizer: trims whitespace; optionally unify suffixes.
    You may customize the mapping below if you want to standardize further.
    """
    if pd.isna(name):
        return None
    s = str(name).strip()
    # Basic canonicalization examples (extend as needed)
    mapping = {
        "内蒙古自治区": "内蒙古自治区",
        "广西壮族自治区": "广西壮族自治区",
        "宁夏回族自治区": "宁夏回族自治区",
        "新疆维吾尔自治区": "新疆维吾尔自治区",
        "西藏自治区": "西藏自治区",
        "北京市": "北京市",
        "天津市": "天津市",
        "上海市": "上海市",
        "重庆市": "重庆市",
    }
    return mapping.get(s, s)

def read_one_file(path: Path, region_col: str = None, clean_region: bool = True) -> Tuple[pd.DataFrame, str]:
    """Read ONE indicator file and reshape to long with columns ['region','year', <varname>]."""
    ext = path.suffix.lower()
    varname = path.stem  # file name without extension
    # Read
    if ext in [".xlsx", ".xls"]:
        df = pd.read_excel(path, engine=None)  # let pandas choose engine
    elif ext == ".csv":
        df = pd.read_csv(path)
    else:
        raise ValueError(f"Unsupported file type: {ext} ({path})")

    # Guess region column if not provided: use the first column by default
    if region_col is None:
        region_col = df.columns[0]
    if region_col not in df.columns:
        raise ValueError(f"Region column '{region_col}' not found in {path}. Available: {list(df.columns)}")

    # Identify year columns (all except region if they parse to a year)
    year_cols = [c for c in df.columns if c != region_col and normalize_year(c) is not None]
    if not year_cols:
        # fallback to heuristic scan
        year_cols = guess_year_columns(df.drop(columns=[region_col]))

    # Keep only region + year columns for melting
    keep_cols = [region_col] + year_cols
    df2 = df[keep_cols].copy()

    # Melt to long
    long_df = df2.melt(id_vars=region_col, var_name="year_raw", value_name=varname)

    # Normalize 'region' and 'year'
    long_df = long_df.rename(columns={region_col: "region"})
    long_df["year"] = long_df["year_raw"].map(normalize_year)

    # Clean region
    if clean_region:
        long_df["region"] = long_df["region"].map(clean_region_name)

    # Coerce numeric values
    long_df[varname] = pd.to_numeric(long_df[varname], errors="coerce")

    # Drop rows with missing region or year (unparsable columns)
    long_df = long_df.dropna(subset=["region", "year"]).drop(columns=["year_raw"])

    # Remove entirely-empty regions if any (all NaN for this variable)
    # We keep them if needed; the full merge later will handle NaNs.
    return long_df, varname

def merge_many_long(dfs: List[pd.DataFrame]) -> pd.DataFrame:
    """Outer-merge on ['region','year'] across many long DFs."""
    if not dfs:
        return pd.DataFrame(columns=["region", "year"])
    out = dfs[0]
    for df in dfs[1:]:
        out = pd.merge(out, df, on=["region", "year"], how="outer")
    # Sort for readability
    out = out.sort_values(["region", "year"]).reset_index(drop=True)
    return out

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--input_dir", type=str, required=True, help="Directory containing indicator files.")
    ap.add_argument("--pattern", type=str, default="*.xlsx", help="Glob pattern, e.g., *.xlsx, *.csv")
    ap.add_argument("--region_col", type=str, default=None, help="Region column name (default: first column).")
    ap.add_argument("--clean_region", type=str, default="yes", choices=["yes","no"], help="Normalize region names lightly.")
    ap.add_argument("--output", type=str, required=True, help="Output file path (.xlsx or .csv).")
    args = ap.parse_args()

    input_dir = Path(args.input_dir)
    paths = sorted(input_dir.glob(args.pattern))
    if not paths:
        print(f"No files matched pattern {args.pattern} in {input_dir}", file=sys.stderr)
        sys.exit(1)

    long_list = []
    for p in paths:
        try:
            long_df, varname = read_one_file(p, region_col=args.region_col, clean_region=(args.clean_region=="yes"))
            print(f"OK: {p.name} -> variable '{varname}', rows={len(long_df)}")
            long_list.append(long_df)
        except Exception as e:
            print(f"FAILED: {p.name}: {e}", file=sys.stderr)

    if not long_list:
        print("No files could be parsed.", file=sys.stderr)
        sys.exit(2)

    panel = merge_many_long(long_list)

    # Save
    out_path = Path(args.output)
    out_path.parent.mkdir(parents=True, exist_ok=True)

    if out_path.suffix.lower() == ".xlsx":
        with pd.ExcelWriter(out_path, engine="openpyxl") as writer:
            panel.to_excel(writer, index=False, sheet_name="panel")
    elif out_path.suffix.lower() == ".csv":
        panel.to_csv(out_path, index=False)
    else:
        print(f"Unsupported output extension: {out_path.suffix}. Use .xlsx or .csv", file=sys.stderr)
        sys.exit(3)

    # Quick diagnostics
    # count duplicates on key
    dup = panel.duplicated(subset=["region","year"]).sum()
    print(f"Saved {len(panel):,} rows, {panel['region'].nunique()} regions, "
          f"{panel['year'].nunique()} years. Duplicated keys: {dup}. Output: {out_path}")

if __name__ == "__main__":
    main()
